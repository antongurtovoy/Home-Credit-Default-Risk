{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Baseline для соревнования Home Credit Default Risk\n",
    "https://www.kaggle.com/c/home-credit-default-risk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import os\n",
    "import pyprind\n",
    "import gc\n",
    "import re\n",
    "import math\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "import copy\n",
    "import time\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder,MinMaxScaler,StandardScaler,PolynomialFeatures\n",
    "from sklearn.model_selection import train_test_split,StratifiedKFold,KFold\n",
    "from sklearn.metrics import roc_auc_score,classification_report\n",
    "\n",
    "from scipy.stats import kendalltau, pearsonr, linregress\n",
    "from pyprind import ProgBar\n",
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.max_columns = 150\n",
    "pd.options.display.max_rows = 150\n",
    "%matplotlib inline\n",
    "warnings.simplefilter('ignore')\n",
    "PATH_TO_DATA = 'D:\\home_bank_data'\n",
    "stat_list = ['mean','median','std','max','min']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_caregorical(columns,count_lim):\n",
    "    placeholder = 'rare_val' if columns.dtype==object else -1\n",
    "    vals_count = columns.value_counts()\n",
    "    vals_count = set(vals_count[vals_count.map(lambda x : x > count_lim )].index)\n",
    "    return columns.map(lambda x : x if x in vals_count else placeholder )\n",
    "\n",
    "generate_name = lambda names,prefix : [ '{}_{}'.format(prefix,name) for name in names ]\n",
    "\n",
    "def calc_stat_fea(groups,columns,stat_list):\n",
    "    \n",
    "    result = pd.DataFrame()\n",
    "    for fea in columns:\n",
    "        if fea not in ['SK_ID_PREV','SK_ID_CURR']:\n",
    "            stats = groups[fea].aggregate(stat_list)\n",
    "            stats.columns = generate_name(stats,fea)\n",
    "            result = pd.concat( (result, stats),axis=1 )\n",
    "    return result\n",
    "\n",
    "def get_last_fea(df,bar,last_name,order_col):\n",
    "    bar.update()\n",
    "    return df.loc[df[order_col].idxmax()][last_name]\n",
    "\n",
    "def extracrt_index_and_diff(df,bar,fea_name,period_count,order_name):\n",
    "    df = df.sort_values(order_name)\n",
    "    placeholder = pd.Series( index = [ 'index_{}_{}'.format(fea_name,val) for val in range(period_count) ] \\\n",
    "                            + ['diff_{}_{}'.format(fea_name,val) for val in range(period_count)] )\n",
    "    if len(df)<period_count+1:\n",
    "        period_count = len(df)-1\n",
    "    fea_col = df[fea_name]\n",
    "    for i in range(0,period_count):\n",
    "        vals_1,vals_2 = fea_col.iloc[-(i+1)],fea_col.iloc[-(i+2)]\n",
    "        placeholder['index_{}_{}'.format(fea_name,i)] = vals_1  / vals_2\n",
    "        placeholder['diff_{}_{}'.format(fea_name,i)] = vals_1 -vals_2\n",
    "    bar.update()\n",
    "    return placeholder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(data,model_type, param,nfolds, fea_list, validate_size = 0.1, stratify=False, seed = 42,\n",
    "             validate_seed = 42, is_debug = True,use_early_stop = False , early_stop_verbose = 10, \n",
    "             early_stopping_rounds = 10):\n",
    "    \n",
    "    #Tracer()()\n",
    "    labeled_data = data[data.TARGET.notnull()]\n",
    "    if not is_debug:\n",
    "        submit_data = data[data.TARGET.isnull()][fea_list]\n",
    "    train_data,validate_data = train_test_split(labeled_data, test_size=validate_size, random_state=validate_seed)\n",
    "    validate_set,validate_target = validate_data[fea_list], validate_data['TARGET']\n",
    "    del validate_data,labeled_data\n",
    "    \n",
    "    splits = KFold(n_splits=nfolds,shuffle=True,random_state=seed)\n",
    "    validate_preditction = np.zeros(len(validate_set))\n",
    "    if not is_debug:\n",
    "        submit_prediction = np.zeros(len(submit_data))\n",
    "    else:\n",
    "        fea_imp = np.zeros((len(fea_list),nfolds))\n",
    "\n",
    "    param = copy.deepcopy(param)\n",
    "    param['random_state'] = seed\n",
    "    oof_score = np.zeros(nfolds)\n",
    "    for i,(train_idx,test_idx) in enumerate(splits.split(train_data)):\n",
    "        \n",
    "        train_set,test_set = train_data.iloc[train_idx][fea_list],train_data.iloc[test_idx][fea_list]\n",
    "        train_target,test_target = train_data.iloc[train_idx]['TARGET'],train_data.iloc[test_idx]['TARGET']\n",
    "        \n",
    "        model = model_type(**param)\n",
    "        if use_early_stop:\n",
    "            model.fit(train_set,train_target,eval_set = [(test_set,test_target)], verbose = early_stop_verbose,\n",
    "                      early_stopping_rounds=early_stopping_rounds, eval_metric='auc')\n",
    "        else:\n",
    "            model.fit(train_set,train_target)\n",
    "        \n",
    "        test_prediction = model.predict_proba(test_set)[:,1]\n",
    "        train_prediction = model.predict_proba(train_set)[:,1]\n",
    "        validate_preditction += model.predict_proba(validate_set)[:,1]\n",
    "        test_auc = roc_auc_score(y_score=test_prediction,y_true=test_target)\n",
    "        oof_score[i] = test_auc \n",
    "        print('fold = {} oof score = {}, train score = {} , validate_score = {} '.format( i+1,\n",
    "            test_auc,\n",
    "            roc_auc_score(y_score=train_prediction,y_true=train_target),\n",
    "            roc_auc_score(y_score=model.predict_proba(validate_set)[:,1],y_true=validate_target)))\n",
    "                        \n",
    "        if not is_debug:\n",
    "            submit_prediction += model.predict_proba(submit_data)[:,1]\n",
    "        else:\n",
    "            fea_imp[:,i] = model.feature_importances_\n",
    "        \n",
    "        del train_set,train_target,test_set,test_target,model\n",
    "        gc.collect()\n",
    "    \n",
    "    validate_preditction/= nfolds \n",
    "    print('with seed {} mean oof = {} , std oof = {}'.format(seed,np.mean(oof_score),np.std(oof_score)))\n",
    "    print('with validation seed {} validate score = {}'.format( validate_seed, \n",
    "                                                    roc_auc_score(y_score=validate_preditction,y_true=validate_target)))\n",
    "    \n",
    "    if not is_debug:\n",
    "        submit_prediction/= nfolds\n",
    "        return pd.Series(submit_prediction,index = submit_data.index)\n",
    "    else:\n",
    "        return fea_imp\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_app_data(train_df,test_df):\n",
    "    \n",
    "    train_df = train_df.copy(deep=True)\n",
    "    train_df.drop(axis=0,index=train_df.query(\n",
    "    '(NAME_FAMILY_STATUS==\"Unknown\")|(CODE_GENDER==\"XNA\")|(NAME_INCOME_TYPE==\"Maternity leave\")').index,inplace=True)\n",
    "    drop_list = [ fea for fea in test_df if test_df[fea].nunique()==1 ]\n",
    "    app_data = pd.concat((train_df,test_df))\n",
    "    app_data.drop(drop_list,axis=1,inplace=True)\n",
    "\n",
    "    bin_fea = [fea for fea in app_data if app_data[fea].nunique()==2 and app_data[fea].dtype==object]\n",
    "    for fea in bin_fea:\n",
    "        app_data.loc[:,fea],_ = pd.factorize(app_data[fea])\n",
    "\n",
    "    app_data.loc[:,'over_sum_amount'] = app_data.AMT_CREDIT - app_data.AMT_GOODS_PRICE\n",
    "    app_data.loc[:,'over_sum_amount'] = app_data['over_sum_amount'].fillna(0)\n",
    "    app_data.loc[:,'relation_amount'] = app_data['over_sum_amount'] / app_data.AMT_CREDIT\n",
    "    app_data.loc[:,'part_of_income'] =  app_data['AMT_ANNUITY'] / app_data['AMT_INCOME_TOTAL']\n",
    "    app_data.loc[:,'part_of_income_per_person'] = (app_data['AMT_INCOME_TOTAL']-app_data['AMT_ANNUITY'])\\\n",
    "                                                /(app_data['CNT_CHILDREN']+1)\n",
    "    app_data.loc[:,'payment_rate'] = app_data['AMT_ANNUITY'] / app_data['AMT_CREDIT']\n",
    "    app_data.loc[:,'rel_size'] = app_data['AMT_CREDIT'] / app_data['AMT_INCOME_TOTAL']\n",
    "    app_data.loc[:,'credit_rate'] = app_data['AMT_CREDIT']  / app_data['AMT_ANNUITY']\n",
    "    app_data.loc[:,'work_rate'] = app_data['DAYS_EMPLOYED'] / app_data['DAYS_BIRTH'] \n",
    "    app_data.loc[:,'price_rate'] = app_data['AMT_CREDIT'] / app_data['AMT_GOODS_PRICE']\n",
    "    app_data.loc[:,'EXT_SOURCE_2 / DAYS_BIRTH'] = app_data['EXT_SOURCE_2'] / app_data['DAYS_BIRTH']\n",
    "    app_data.loc[:,'EXT_SOURCE_1*EXT_SOURCE_2'] = app_data['EXT_SOURCE_1']*app_data['EXT_SOURCE_2']\n",
    "    app_data.loc[:,'DAYS_EMPLOYED-DAYS_BIRTH'] = app_data['DAYS_EMPLOYED'] - app_data['DAYS_BIRTH']\n",
    "    ext_name = ['EXT_SOURCE_1','EXT_SOURCE_2','EXT_SOURCE_3']\n",
    "\n",
    "    app_data.loc[:,'ext_means'] = app_data[ext_name].mean(axis=1)\n",
    "    app_data.loc[:,'ext_std'] = app_data[ext_name].std(axis=1)\n",
    "    app_data.loc[:,'delta_work_both'] = app_data['DAYS_EMPLOYED'] - app_data['DAYS_BIRTH']\n",
    "\n",
    "    dummy_col = [ fea for fea in app_data if app_data[fea].dtype==object ]\n",
    "    app_data = pd.get_dummies(app_data,columns=dummy_col)\n",
    "    app_data.set_index('SK_ID_CURR',inplace=True)\n",
    "    return app_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bureau_and_balance_data(df_bureau,balance_data):\n",
    "    \n",
    "    df_bureau = df_bureau.copy(deep=True)\n",
    "    df_bureau.loc[:,'credit_period_waited'] = df_bureau.DAYS_CREDIT_ENDDATE - df_bureau.DAYS_CREDIT\n",
    "    df_bureau.loc[:,'credit_period_fact'] = df_bureau.DAYS_ENDDATE_FACT - df_bureau.DAYS_CREDIT\n",
    "    df_bureau.loc[:,'debt'] =  (df_bureau.AMT_CREDIT_SUM - df_bureau.AMT_CREDIT_SUM_DEBT)/ \\\n",
    "                            df_bureau.AMT_CREDIT_SUM\n",
    "    df_bureau.loc[:,'annuity'] = df_bureau.AMT_CREDIT_SUM / (df_bureau.credit_period_waited/30.4)\n",
    "    df_bureau.loc[:,'payment_rate'] = df_bureau.annuity / df_bureau.AMT_CREDIT_SUM\n",
    "    df_bureau.loc[:,'diffs_period'] = df_bureau.credit_period_waited - df_bureau.credit_period_fact\n",
    "    df_bureau.loc[:,'rate_period'] = df_bureau.credit_period_waited / df_bureau.credit_period_fact\n",
    "    \n",
    "    df_bureau = pd.get_dummies(df_bureau,columns=['CREDIT_TYPE','CREDIT_ACTIVE'])\n",
    "    dummy_name = [ fea for fea in df_bureau if re.match('CREDIT_TYPE',fea) or re.match('CREDIT_ACTIVE',fea) ] \n",
    "    \n",
    "    group = df_bureau.groupby(by='SK_ID_CURR')\n",
    "    cat_sum = group[dummy_name].sum()\n",
    "    cat_sum.columns = generate_name(cat_sum.columns,'sum')\n",
    "    cat_mean = group[dummy_name].mean()\n",
    "    cat_mean.columns = generate_name(cat_mean.columns,'mean')\n",
    "    last_update = group['DAYS_CREDIT_UPDATE'].max()\n",
    "    last_update.name= 'last_update'\n",
    "    ltv = group['DAYS_CREDIT'].min() - group['DAYS_CREDIT_UPDATE'].max()\n",
    "    ltv.name = 'ltv'\n",
    "    annuity_diff = group['annuity'].max() - group['annuity'].min()\n",
    "    annuity_diff.name = 'annuity_diff'\n",
    "    payment_rate_diff = group['payment_rate'].max() - group['payment_rate'].min()\n",
    "    payment_rate_diff.name = 'payment_rate_diff'\n",
    "    sum_overdue = group['AMT_CREDIT_SUM_OVERDUE'].sum()\n",
    "    sum_overdue.name = 'sum_overdue'\n",
    "\n",
    "    data_without_card = df_bureau[df_bureau['CREDIT_TYPE_Credit card']==0]\n",
    "    group = data_without_card.groupby(by='SK_ID_CURR')\n",
    "    credit_count  = group['SK_ID_BUREAU'].size()\n",
    "    credit_count.name = 'credit_count'\n",
    "    non_card_stat_fea_name = ['credit_period_waited','credit_period_fact','debt','annuity','payment_rate',\n",
    "                          'AMT_CREDIT_SUM','AMT_CREDIT_SUM_DEBT','DAYS_CREDIT_UPDATE','diffs_period','rate_period']\n",
    "    stat_name = ['min','max','median','mean','std']\n",
    "    non_card_fea = calc_stat_fea(group,non_card_stat_fea_name,stat_name)\n",
    "    non_card_fea.columns = generate_name(non_card_fea.columns,'non_card_fea')\n",
    "\n",
    "    bar = ProgBar(len(group),title='extract non-card last fea')\n",
    "    non_card_last = group.apply(get_last_fea,bar,non_card_stat_fea_name,'DAYS_CREDIT_UPDATE')\n",
    "    non_card_last.columns = generate_name(non_card_last.columns,'last_non_card_fea')\n",
    "\n",
    "    card_data = df_bureau[df_bureau['CREDIT_TYPE_Credit card']==1]\n",
    "    group = card_data.groupby(by='SK_ID_CURR')\n",
    "    card_count = group.size()\n",
    "    card_count.name = 'card_count'\n",
    "    card_fea_name = ['debt','AMT_CREDIT_SUM','AMT_CREDIT_SUM_DEBT']\n",
    "    card_fea = calc_stat_fea(group,card_fea_name,stat_name)\n",
    "    card_fea.columns = generate_name(card_fea.columns,'card_fea')\n",
    "\n",
    "    bar = ProgBar(len(group),title='extract card last fea')\n",
    "    card_last = card_data.groupby(by='SK_ID_CURR').apply(get_last_fea,bar,card_fea_name,'DAYS_CREDIT_UPDATE')\n",
    "    card_last.columns = generate_name(card_last.columns,'last')\n",
    "    \n",
    "    balance_data = balance_data.merge(right=df_bureau[['SK_ID_BUREAU','SK_ID_CURR']],\n",
    "                          right_on='SK_ID_BUREAU',left_on = 'SK_ID_BUREAU',how='left')\n",
    "    balance_data = pd.get_dummies(balance_data,columns=['STATUS'])\n",
    "    bureau_balance_dummy_name = [ fea for fea in balance_data if re.match('STATUS',fea)]\n",
    "    group = balance_data.groupby(by='SK_ID_CURR')\n",
    "    sum_status = group[bureau_balance_dummy_name].sum()\n",
    "    sum_status.columns = generate_name(sum_status.columns,'sum_status')\n",
    "    mean_status = group[bureau_balance_dummy_name].mean()\n",
    "    mean_status.columns = generate_name(mean_status.columns,'mean_status')\n",
    "    \n",
    "    bureau_df = pd.concat( (cat_sum,cat_mean, last_update, ltv,credit_count, non_card_fea, annuity_diff,\n",
    "                            payment_rate_diff, sum_overdue, card_count, card_fea, card_last,non_card_last,\n",
    "                        sum_status,mean_status, ),axis=1 )\n",
    "    bureau_df.columns = generate_name(bureau_df.columns,'bureau')\n",
    "    return bureau_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_previous_application_features(df_prev):\n",
    "\n",
    "    df_prev = df_prev.copy(deep=True)\n",
    "    df_prev.loc[:,'diff_amt_credit_amt_good_price'] = df_prev.AMT_CREDIT - df_prev.AMT_GOODS_PRICE\n",
    "    df_prev.loc[:,'diff_amt_credit_amt_application']= df_prev.AMT_CREDIT - df_prev.AMT_APPLICATION\n",
    "    df_prev.loc[:,'diff_amt_application_amt_good_price'] = df_prev.AMT_APPLICATION - df_prev.AMT_GOODS_PRICE\n",
    "    df_prev.loc[:,'pure_sum'] = df_prev.AMT_GOODS_PRICE - df_prev.AMT_DOWN_PAYMENT.fillna(0)\n",
    "    df_prev.loc[:,'diff_credit_pure'] = df_prev.AMT_CREDIT - df_prev.pure_sum\n",
    "    df_prev.loc[:,'payment_rate'] = df_prev.AMT_ANNUITY  / df_prev.AMT_CREDIT\n",
    "    df_prev.loc[:,'total_sum'] = df_prev.CNT_PAYMENT * df_prev.AMT_ANNUITY\n",
    "    df_prev.loc[:,'diff_total_sum_good_price'] = df_prev.total_sum - df_prev.AMT_GOODS_PRICE\n",
    "    df_prev.loc[:,'diff_total_sum_amt_credit'] = df_prev.total_sum - df_prev.AMT_CREDIT\n",
    "    df_prev.loc[:,'diff_total_sum_amt_application'] = df_prev.total_sum  - df_prev.AMT_APPLICATION\n",
    "    df_prev.loc[:,'diff_total_sum_pure_sum'] = df_prev.total_sum -df_prev.pure_sum\n",
    "    df_prev.loc[:,'concat_STATUS_TYPE'] = df_prev['NAME_CONTRACT_STATUS'] + df_prev['NAME_CONTRACT_TYPE']\n",
    "    df_prev.loc[:,'interest_rate'] = (df_prev.total_sum - df_prev.pure_sum)\\\n",
    "                    / (df_prev.CNT_PAYMENT*df_prev.total_sum)\n",
    "    df_prev.loc[:,'delta_days_waited'] = df_prev.DAYS_LAST_DUE_1ST_VERSION - df_prev.DAYS_FIRST_DUE\n",
    "    df_prev.loc[:,'delta_days_fact'] = df_prev.DAYS_LAST_DUE - df_prev.DAYS_FIRST_DRAWING\n",
    "    df_prev.loc[:,'diff_waited_fact'] = df_prev.DAYS_LAST_DUE_1ST_VERSION - df_prev.DAYS_LAST_DUE\n",
    "    df_prev.loc[:,'rate_fact_waited'] = df_prev.DAYS_LAST_DUE_1ST_VERSION / df_prev.DAYS_LAST_DUE\n",
    "\n",
    "    approved_data = df_prev.query('(NAME_CONTRACT_TYPE!=\"Revolving loans\")&(NAME_CONTRACT_STATUS==\"Approved\")')\n",
    "    app_group = approved_data.groupby(by='SK_ID_CURR')\n",
    "\n",
    "    bar = ProgBar(approved_data.SK_ID_CURR.nunique(),title='extract payment index')\n",
    "    index_payment_rate = app_group.apply(extracrt_index_and_diff,bar,'payment_rate',4,'DAYS_DECISION')\n",
    "\n",
    "    bar = ProgBar(approved_data.SK_ID_CURR.nunique(),title = 'extract interest rate')\n",
    "    index_interest_rate = app_group.apply(extracrt_index_and_diff,bar,'interest_rate',4,'DAYS_DECISION')\n",
    "\n",
    "    stat_agg_name = ['diff_amt_application_amt_good_price','diff_credit_pure','pure_sum','payment_rate',\n",
    "                     'diff_total_sum_amt_credit','diff_total_sum_amt_application','total_sum',\n",
    "                     'diff_total_sum_pure_sum', 'AMT_ANNUITY','interest_rate','delta_days_waited','delta_days_fact',\n",
    "                     'diff_waited_fact','rate_fact_waited','CNT_PAYMENT']\n",
    "\n",
    "    group = df_prev.groupby(by='SK_ID_CURR')\n",
    "    aggregate_data = calc_stat_fea(group,stat_agg_name,stat_list=stat_list)\n",
    "\n",
    "    ltv = group['DAYS_DECISION'].min() - group['DAYS_TERMINATION'].max()\n",
    "    ltv.name = 'ltv'\n",
    "\n",
    "    last_name = ['diff_total_sum_pure_sum','interest_rate','diff_total_sum_amt_application',\n",
    "                 'diff_total_sum_amt_credit','diff_total_sum_good_price',\n",
    "                 'total_sum','payment_rate','pure_sum','diff_amt_application_amt_good_price',\n",
    "                 'diff_amt_credit_amt_application','diff_amt_credit_amt_good_price','DAYS_TERMINATION',\n",
    "                 'HOUR_APPR_PROCESS_START','DAYS_DECISION','rate_fact_waited','diff_waited_fact','delta_days_fact',\n",
    "                 'AMT_ANNUITY']\n",
    "    bar = ProgBar(len(app_group),title = 'extract last fea')\n",
    "    last_fea = app_group.apply(get_last_fea,bar,last_name,order_col='DAYS_DECISION')\n",
    "    last_fea.columns = generate_name(last_fea.columns,'last')\n",
    "\n",
    "    revolve_name = ['AMT_ANNUITY','AMT_CREDIT','payment_rate']\n",
    "    revolve_data = df_prev.query('(NAME_CONTRACT_STATUS==\"Approved\")&(NAME_CONTRACT_TYPE==\"Revolving loans\")')\n",
    "    bar = ProgBar(revolve_data.SK_ID_CURR.nunique(),'extract revolve fea')\n",
    "    revolv_fea = revolve_data.groupby(by='SK_ID_CURR').apply(get_last_fea,bar,revolve_name,'DAYS_DECISION')\n",
    "    revolv_fea.columns = generate_name(revolv_fea.columns,'revolve')\n",
    "    \n",
    "    dummy_col = ['NAME_CONTRACT_STATUS','NAME_CONTRACT_TYPE','concat_STATUS_TYPE']\n",
    "    dummy_data = pd.get_dummies(df_prev[dummy_col+['SK_ID_CURR']],columns=dummy_col)\n",
    "    dummy_group = dummy_data.groupby('SK_ID_CURR')\n",
    "    dummy_sum = dummy_group.sum()\n",
    "    dummy_sum.columns = generate_name(dummy_sum.columns,'sum')\n",
    "    dummy_mean = dummy_group.mean()\n",
    "    dummy_mean.columns = generate_name(dummy_mean.columns,'mean')\n",
    "    \n",
    "    result = pd.concat( (index_payment_rate,index_interest_rate,aggregate_data,ltv,last_fea,revolv_fea, \n",
    "                         dummy_mean,dummy_sum) ,axis=1)\n",
    "    result.columns = generate_name(result.columns,'prev')\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "fea_names = ['payment_diff','count_over_instalment','amt_over','amt_incomplete',\n",
    "             'over_rate','incompete_rate','total_instalment','over_count_rate']\n",
    "\n",
    "def extract_payment_fea(df,bar):                           \n",
    "    amt_instalment = df.groupby(by=['SK_ID_PREV','DAYS_INSTALMENT'])['AMT_INSTALMENT'].last()\n",
    "    total_instalment = amt_instalment.sum()    \n",
    "    total_payment = df['AMT_PAYMENT'].sum()\n",
    "    payment_diffs = total_instalment - total_payment\n",
    "    bad_payment_sum = df.query('delta_days<0')['AMT_PAYMENT'].sum()\n",
    "    over_payment_fea = df.query( '(delta_days>0) & (AMT_PAYMENT>AMT_INSTALMENT)' )['AMT_PAYMENT']\n",
    "    over_payment_fea.name = 'AMT_OVER'\n",
    "    add_fea = []\n",
    "    for col in [amt_instalment,df['AMT_PAYMENT'],over_payment_fea,]:\n",
    "        stats = col.aggregate(stat_list)\n",
    "        stats.index = [ '{}_{}'.format(name,col.name) for name in stat_list ]\n",
    "        add_fea.append(stats)\n",
    "    over_payment_count = len(over_payment_fea)\n",
    "    over_payment_sum = over_payment_fea.sum()\n",
    "    over_rate = over_payment_sum / total_instalment\n",
    "    incompete_rate = bad_payment_sum / total_instalment\n",
    "    over_count_rate = over_payment_count / len(amt_instalment)\n",
    "    bar.update()\n",
    "    add_fea.append(  pd.Series( \n",
    "        [payment_diffs,over_payment_count, over_payment_sum,bad_payment_sum,over_rate,\n",
    "         incompete_rate,total_instalment,over_count_rate],\n",
    "        index=fea_names))\n",
    "    return pd.concat(add_fea)\n",
    "\n",
    "def get_payment_features(df_payment):\n",
    "    df_payment.loc[:,'delta_days'] =  df_payment['DAYS_INSTALMENT'] - df_payment['DAYS_ENTRY_PAYMENT']\n",
    "    df_payment.loc[:,'bad_payment'] = df_payment.delta_days <0\n",
    "\n",
    "    stat_name = ['mean','median','std','max','min']\n",
    "    payment_group = df_payment.groupby(by='SK_ID_CURR')\n",
    "    days = calc_stat_fea(payment_group,['delta_days'],stat_list)\n",
    "    bad_payment_count = payment_group['bad_payment'].sum()\n",
    "    bad_payment_rate = payment_group['bad_payment'].mean()\n",
    "    bad_payment_count.name = 'bad_payment_count'\n",
    "    bad_payment_rate.name = 'bad_payment_rate'\n",
    "    num_unique_inst = payment_group['NUM_INSTALMENT_VERSION'].nunique()\n",
    "    ltv = (payment_group['DAYS_ENTRY_PAYMENT'].max() -  payment_group['DAYS_INSTALMENT'].min()).map(abs)\n",
    "    ltv.name = 'life_time_value'\n",
    "    last_day = payment_group['DAYS_ENTRY_PAYMENT'].max()\n",
    "    last_day.name = 'last_entry_payment'\n",
    "    last_delta_day = payment_group['DAYS_ENTRY_PAYMENT'].max() -  payment_group['DAYS_INSTALMENT'].max()\n",
    "    last_delta_day.name = 'last_delta_day'\n",
    "\n",
    "    bar = ProgBar(payment_data.SK_ID_CURR.nunique())\n",
    "    correlation = payment_group.apply(extract_corr,bar)\n",
    "    \n",
    "    non_card_payment = df_payment.query('(NUM_INSTALMENT_VERSION!=0) & (AMT_INSTALMENT!=0)')\n",
    "    bar = ProgBar(non_card_payment.SK_ID_CURR.nunique())\n",
    "    non_card_payment_fea = non_card_payment.groupby(by = 'SK_ID_CURR').apply(extract_payment_fea,bar)\n",
    "    non_card_payment_fea.columns = generate_name(non_card_payment_fea.columns,'non_card')\n",
    "\n",
    "    card_payment = df_payment.query('(NUM_INSTALMENT_VERSION==0) & (AMT_INSTALMENT!=0)')\n",
    "    bar = ProgBar(card_payment.SK_ID_CURR.nunique())\n",
    "    card_payment_fea = card_payment.groupby(by = 'SK_ID_CURR').apply(extract_payment_fea,bar)\n",
    "    card_payment_fea.columns = generate_name(card_payment_fea.columns,'card')\n",
    "\n",
    "    payment_agg_data = df_payment.query('(AMT_INSTALMENT!=0)')\n",
    "    bar = ProgBar(payment_agg_data.SK_ID_CURR.nunique())\n",
    "    total_payment_fea = payment_agg_data.groupby(by='SK_ID_CURR').apply(extract_payment_fea,bar)\n",
    "    total_payment_fea.columns = generate_name(total_payment_fea.columns,'total')\n",
    "    last_payment_dict = OrderedDict()\n",
    "    for i in [-10,-30,-50,-100,-200,-300,-400,-500,]:\n",
    "        last_data = payment_data.query('DAYS_ENTRY_PAYMENT>={}'.format(i)).groupby('SK_ID_CURR')\n",
    "        count_last_payments = last_data.size()\n",
    "        count_bad_payment = last_data['bad_payment'].sum()\n",
    "        bad_rate = count_bad_payment / count_last_payments\n",
    "        last_payment_dict['last_payment_{}'.format(i)] = count_last_payments\n",
    "        last_payment_dict['last_bad_payment_{}'.format(i)] = count_bad_payment\n",
    "        last_payment_dict['last_bad_rate_{}'.format(i)] = bad_rate\n",
    "    last_payment_df = pd.DataFrame(last_payment_dict)\n",
    "    \n",
    "    result = pd.concat( (days,bad_payment_count,bad_payment_rate,bad_payment_count,num_unique_inst,\n",
    "                       ltv,last_day,last_delta_day,\n",
    "                         correlation,\n",
    "                         non_card_payment_fea,\n",
    "                         card_payment_fea,\n",
    "                         total_payment_fea,\n",
    "                        last_payment_df),axis=1  )\n",
    "    result.columns = generate_name(result.columns,'payment_data')\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pos_features(df_pos):\n",
    "    df_pos = pd.get_dummies(df_pos,columns=['NAME_CONTRACT_STATUS'],dummy_na=True)\n",
    "    dummy_col = [fea for fea in df_pos if re.match('NAME_CONTRACT_STATUS',fea)]\n",
    "    pos_group = df_pos.groupby(by='SK_ID_CURR')\n",
    "    state_sum = pos_group[dummy_col].sum()\n",
    "    state_sum.columns = generate_name(state_sum.columns,'sum')\n",
    "    state_mean = pos_group[dummy_col].mean()\n",
    "    state_mean.columns = generate_name(state_mean.columns,'mean')\n",
    "    size = pos_group.size()\n",
    "    size.name = 'record_count'\n",
    "    dpd_days = calc_stat_fea(pos_group,['SK_DPD','SK_DPD_DEF'],stat_list=stat_list)\n",
    "    count_zero_instalment = pos_group['CNT_INSTALMENT_FUTURE'].apply( lambda df: np.sum(df==0) )\n",
    "    count_zero_instalment.name = 'count_zero_instalment'\n",
    "    mean_zero_instalment = count_zero_instalment / size\n",
    "    mean_zero_instalment.name = 'mean_zero_instalment'\n",
    "    pos_ltv = pos_group['MONTHS_BALANCE'].min() - pos_group['MONTHS_BALANCE'].max()\n",
    "    pos_ltv.name = 'ltv'\n",
    "    pos_last_date = pos_group['MONTHS_BALANCE'].max()\n",
    "    pos_last_date.name = 'last_date'\n",
    "    post_last_CNT_INSTALMENT = pos_group[['CNT_INSTALMENT_FUTURE','MONTHS_BALANCE']].apply( lambda df : \n",
    "                                        df.loc[df['MONTHS_BALANCE'].idxmax()]['CNT_INSTALMENT_FUTURE'] )\n",
    "    post_last_CNT_INSTALMENT.name = 'last_CNT_INSTALMENT'\n",
    "    pos_date = calc_stat_fea(pos_group,['MONTHS_BALANCE'],stat_list)\n",
    "    future_payment_fea = df_pos.query('(MONTHS_BALANCE==-1)&(CNT_INSTALMENT_FUTURE!=0)').groupby(\n",
    "        by='SK_ID_CURR')['CNT_INSTALMENT_FUTURE'].size()\n",
    "    future_payment_fea.name = 'count_open_credit'\n",
    "    pos_count_credit = pos_group['SK_ID_PREV'].nunique()\n",
    "    pos_count_credit.name = 'count_credit'\n",
    "    cnt_instalment_stats = calc_stat_fea(pos_group,['CNT_INSTALMENT_FUTURE'],stat_list)\n",
    "    cnt_instalment = calc_stat_fea(pos_group,['CNT_INSTALMENT'],stat_list)\n",
    "    pos_features = pd.concat(  (\n",
    "        state_mean,\n",
    "        state_sum,\n",
    "        size,\n",
    "        mean_zero_instalment,\n",
    "        count_zero_instalment,\n",
    "        dpd_days,\n",
    "        pos_ltv,\n",
    "        pos_date,\n",
    "        post_last_CNT_INSTALMENT,\n",
    "        cnt_instalment,\n",
    "        future_payment_fea,\n",
    "        pos_count_credit,\n",
    "        cnt_instalment_stats,\n",
    "    ),axis=1)\n",
    "    pos_features.columns = generate_name(pos_features.columns,'pos')\n",
    "    return pos_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_name = ['slope','intercept']\n",
    "def extract_trend_fea(df,bar,last,fea_name):\n",
    "    names = [ '{}_{}'.format(fea_name,name) for name in reg_name ] \n",
    "    if last!=-1:\n",
    "        names = ['last_{}_{}'.format(last,name) for name in names ]\n",
    "    df = df.sort_values('MONTHS_BALANCE').groupby(by='MONTHS_BALANCE').sum()[fea_name].reset_index()\n",
    "    if last!=-1 and last>len(df):\n",
    "        last = len(df)\n",
    "        df = df[-last:]\n",
    "    vals = StandardScaler().fit_transform(df.values)\n",
    "    regres = linregress(vals)\n",
    "    bar.update()\n",
    "    return pd.Series( [regres.slope,regres.intercept] ,\n",
    "                     index=names )\n",
    "\n",
    "def extract_corr(df,bar):\n",
    "    col_1,col_2 = df['DAYS_INSTALMENT'],df['DAYS_ENTRY_PAYMENT']\n",
    "    corr_k_days = kendalltau(col_1,col_2)[0]\n",
    "    corr_p_days = pearsonr(col_1,col_2)[0]\n",
    "    col_1,col_2 = df['AMT_PAYMENT'],df['AMT_INSTALMENT']\n",
    "    corr_k_amt = kendalltau(col_1,col_2)[0]\n",
    "    corr_p_amt = pearsonr(col_1,col_2)[0]\n",
    "    bar.update()\n",
    "    return pd.Series( [corr_k_days,corr_p_days,corr_k_amt,corr_p_amt],\n",
    "                      index = ['days_corr_kend','days_corr_pear','amt_corr_kend','amt_corr_kend'])\n",
    "\n",
    "def balance_change_extract(df,bar):\n",
    "    df = df.sort_values(by='MONTHS_BALANCE')\n",
    "    data = df.groupby(by='MONTHS_BALANCE')[['AMT_BALANCE','AMT_RECEIVABLE_PRINCIPAL','AMT_RECIVABLE']].sum()\n",
    "    fea_dict = OrderedDict()\n",
    "    temp = data.diff(1)\n",
    "    fea_dict['corr_balance_principal'] = kendalltau(data['AMT_BALANCE'],data['AMT_RECEIVABLE_PRINCIPAL'])[0]\n",
    "    fea_dict['corr_balance_RECEIVABLE'] = kendalltau(data['AMT_BALANCE'],data['AMT_RECIVABLE'])[0]\n",
    "    for fea in temp:\n",
    "        for func in [np.mean,np.max,np.std,np.min]:\n",
    "            fea_dict['diff_{}_{}'.format(fea,func.__name__)] = func(data[fea])\n",
    "    bar.update()\n",
    "    return pd.Series(fea_dict)\n",
    "\n",
    "def pos_atm_correlation(df,bar):\n",
    "    atm_correlation = pearsonr(df['AMT_DRAWINGS_CURRENT'],df['AMT_DRAWINGS_ATM_CURRENT'])[0]\n",
    "    pos_correlation = pearsonr(df['AMT_DRAWINGS_CURRENT'],df['AMT_DRAWINGS_POS_CURRENT'])[0]\n",
    "    other_correlation = pearsonr(df['AMT_DRAWINGS_CURRENT'],df['AMT_DRAWINGS_POS_CURRENT'])[0]\n",
    "    bar.update()\n",
    "    return pd.Series([atm_correlation,pos_correlation,other_correlation],\n",
    "                    index = ['atm_correlation','pos_correlation','other_correlation'])\n",
    "\n",
    "card_last_name = ['last_AMT_CREDIT_LIMIT_ACTUAL','last_AMT_BALANCE','last_index_AMT_BALANCE',\n",
    "                  'last_diff_AMT_BALANCE']\n",
    "def card_last_balance_data(df,bar):\n",
    "    df = df.sort_values(by='MONTHS_BALANCE')\n",
    "    last_record = df.iloc[-1]\n",
    "    last_debt = last_record['AMT_BALANCE']\n",
    "    last_limit = last_record['AMT_CREDIT_LIMIT_ACTUAL']\n",
    "    last_index = np.nan\n",
    "    last_diff = np.nan\n",
    "    if len(df)>1:\n",
    "        balance = df.iloc[-2]['AMT_BALANCE']\n",
    "        last_index =  last_debt/ balance\n",
    "        last_diff = last_debt - balance\n",
    "    bar.update()\n",
    "    return pd.Series( [last_limit,last_debt,last_index,last_diff] ,index = card_last_name )\n",
    "\n",
    "def get_credit_card_data(credit_card_balance):\n",
    "    \n",
    "    credit_card_balance = pd.get_dummies(credit_card_balance,columns=['NAME_CONTRACT_STATUS'])\n",
    "    dummy_name = [fea for fea in credit_card_balance if re.match('NAME_CONTRACT_STATUS',fea)]\n",
    "    credit_card_balance.loc[:,'atm_diff'] = credit_card_balance.AMT_DRAWINGS_CURRENT - \\\n",
    "                                            credit_card_balance.AMT_DRAWINGS_ATM_CURRENT\n",
    "    credit_card_balance.loc[:,'pos_diff'] = credit_card_balance.AMT_DRAWINGS_CURRENT - \\\n",
    "                                            credit_card_balance.AMT_DRAWINGS_POS_CURRENT\n",
    "    credit_card_balance.loc[:,'other_diff'] = credit_card_balance.AMT_DRAWINGS_CURRENT - \\\n",
    "                                              credit_card_balance.AMT_DRAWINGS_OTHER_CURRENT\n",
    "    credit_card_balance.loc[:,'payment_diff'] = credit_card_balance.AMT_PAYMENT_TOTAL_CURRENT - \\\n",
    "                                                credit_card_balance.AMT_PAYMENT_CURRENT\n",
    "    credit_card_balance.loc[:,'amt_debt'] = credit_card_balance.AMT_BALANCE - credit_card_balance.AMT_RECIVABLE\n",
    "    credit_card_balance.loc[:,'amt_debt_PRINCIPAL']  = credit_card_balance.AMT_BALANCE - \\\n",
    "                                                       credit_card_balance.AMT_RECEIVABLE_PRINCIPAL\n",
    "    credit_card_balance.loc[:,'amt_debt_TOTAL'] = credit_card_balance.AMT_BALANCE - \\\n",
    "                                                          credit_card_balance.AMT_TOTAL_RECEIVABLE\n",
    "    credit_card_balance.loc[:,'diff_pincipal_receivable'] = credit_card_balance.AMT_RECEIVABLE_PRINCIPAL -\\\n",
    "                                                            credit_card_balance.AMT_RECIVABLE    \n",
    "    credit_card_balance.loc[:,'card_payment_rate'] = credit_card_balance.AMT_BALANCE -\\\n",
    "                                                        credit_card_balance.AMT_PAYMENT_CURRENT\n",
    "    \n",
    "    bar = ProgBar(credit_card_balance.SK_ID_CURR.nunique())\n",
    "    corr_data = credit_card_balance.groupby(by='SK_ID_CURR')[['AMT_DRAWINGS_CURRENT',\n",
    "                   'AMT_DRAWINGS_ATM_CURRENT','AMT_DRAWINGS_POS_CURRENT' ]].apply(pos_atm_correlation,bar)\n",
    "    \n",
    "    groups = credit_card_balance.groupby(by='SK_ID_CURR')\n",
    "    mean_status = groups[dummy_name].mean()\n",
    "    mean_status.columns = generate_name(mean_status.columns,'mean')\n",
    "    sum_status = groups[dummy_name].sum()\n",
    "    sum_status.columns = generate_name(sum_status.columns,'sum')\n",
    "    card_ltv = groups.MONTHS_BALANCE.min() - groups.MONTHS_BALANCE.max()\n",
    "    card_ltv.name = 'card_life_time_value'\n",
    "    total_card_count = groups['SK_ID_PREV'].nunique()\n",
    "    total_card_count.name = 'total_card_count'\n",
    "    total_record_count = groups.size()\n",
    "    total_record_count.name = 'card_total_record_count'\n",
    "    cnt_instalment_sum = groups['CNT_INSTALMENT_MATURE_CUM'].max()\n",
    "    cnt_instalment_sum.name = 'cnt_instalment_sum'\n",
    "    \n",
    "    diff_name = ['atm_diff','pos_diff','other_diff','amt_debt','amt_debt_PRINCIPAL','amt_debt_TOTAL',\n",
    "                 'diff_pincipal_receivable','AMT_BALANCE','SK_DPD','AMT_CREDIT_LIMIT_ACTUAL',\n",
    "                 'CNT_DRAWINGS_CURRENT']\n",
    "    stat_diff_fea = calc_stat_fea(groups,diff_name,stat_list)\n",
    "    \n",
    "    bar = ProgBar(credit_card_balance.SK_ID_CURR.nunique())\n",
    "    actual_credit_linmit_data = groups[['MONTHS_BALANCE','AMT_CREDIT_LIMIT_ACTUAL']].apply(\n",
    "        extract_trend_fea,bar,-1,'AMT_CREDIT_LIMIT_ACTUAL',)\n",
    "    \n",
    "    bar = ProgBar(credit_card_balance.SK_ID_CURR.nunique())\n",
    "    last_credit_card_data = groups[['MONTHS_BALANCE','AMT_BALANCE',\n",
    "                                    'AMT_CREDIT_LIMIT_ACTUAL']].apply( card_last_balance_data, bar )\n",
    "    \n",
    "    drawing_sum = groups.AMT_DRAWINGS_CURRENT.sum()\n",
    "    rate_fea_dict = OrderedDict()\n",
    "    for name,fea_name in zip(['pos_rate','atm_rate','other_rate'],['AMT_DRAWINGS_POS_CURRENT',\n",
    "                                            'AMT_DRAWINGS_ATM_CURRENT','AMT_DRAWINGS_OTHER_CURRENT']):\n",
    "        fea = groups[fea_name].sum() / drawing_sum\n",
    "        fea.name = name\n",
    "        rate_fea_dict[name] = fea\n",
    "    \n",
    "    rates_diffs = rate_fea_dict['pos_rate']  - rate_fea_dict['atm_rate']\n",
    "    rates_diffs.name = 'diffs_pos_atm_rate'\n",
    "    \n",
    "    rate_fea_dict['diffs_pos_atm_rate'] = rates_diffs\n",
    "    rate_frame = pd.DataFrame(rate_fea_dict)\n",
    "    \n",
    "    bar = ProgBar(credit_card_balance.SK_ID_CURR.nunique())\n",
    "    credit_balancer_fea = groups.apply(balance_change_extract,bar)\n",
    "    \n",
    "    df_balance_credit = pd.concat((\n",
    "        mean_status,sum_status,\n",
    "        card_ltv,\n",
    "        stat_diff_fea,\n",
    "        total_record_count,\n",
    "        total_card_count,\n",
    "        rate_frame,\n",
    "        cnt_instalment_sum,\n",
    "        credit_balancer_fea,\n",
    "        corr_data,\n",
    "        actual_credit_linmit_data,\n",
    "        last_credit_card_data,\n",
    "        ),axis=1)\n",
    "    df_balance_credit.columns = generate_name(df_balance_credit.columns,'crc')\n",
    "    return df_balance_credit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_df_and_get_features(func_name,*args):\n",
    "    datataset_list = []\n",
    "    for name in args:\n",
    "        datataset_list.append(pd.read_csv(os.path.join(PATH_TO_DATA,name)))\n",
    "    features = func_name(*datataset_list)\n",
    "    del datataset_list\n",
    "    gc.collect()\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "extract non-card last fea\n",
      "0% [##############################] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:04:18\n",
      "\n",
      "Total time elapsed: 00:04:18\n",
      "extract card last fea\n",
      "0% [##############################] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:02:51\n",
      "\n",
      "Total time elapsed: 00:02:51\n",
      "extract payment index\n",
      "0% [##############################] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:03:59\n",
      "\n",
      "Total time elapsed: 00:03:59\n",
      "extract interest rate\n",
      "0% [##############################] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:03:53\n",
      "\n",
      "Total time elapsed: 00:03:53\n",
      "extract last fea\n",
      "0% [##############################] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:05:00\n",
      "\n",
      "Total time elapsed: 00:05:00\n",
      "0% [##############################] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:01:15\n",
      "\n",
      "Total time elapsed: 00:01:15\n",
      "0% [##############################] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:00:56\n",
      "\n",
      "Total time elapsed: 00:00:56\n",
      "0% [##############################] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:05:18\n",
      "\n",
      "Total time elapsed: 00:05:18\n",
      "0% [##############################] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:01:20\n",
      "\n",
      "Total time elapsed: 00:01:20\n",
      "0% [##############################] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:06:30\n",
      "\n",
      "Total time elapsed: 00:06:30\n"
     ]
    }
   ],
   "source": [
    "app_features =   load_df_and_get_features(get_app_data,'application_train.csv','application_test.csv')\n",
    "bureau_balance_feature = load_df_and_get_features(bureau_and_balance_data,'bureau.csv','bureau_balance.csv')\n",
    "pos_features = load_df_and_get_features(get_pos_features,'POS_CASH_balance.csv')\n",
    "prev_app_features = load_df_and_get_features(get_previous_application_features,'previous_application.csv')\n",
    "credit_balance_features = load_df_and_get_features(get_credit_card_data,'credit_card_balance.csv')\n",
    "payment_features = load_df_and_get_features(get_payment_features,'installments_payments.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_features = pd.concat( (app_features,bureau_balance_feature,\n",
    "                          pos_features,prev_app_features,credit_balance_features,payment_features),\n",
    "                        axis=1 )\n",
    "del app_features,bureau_balance_feature,pos_features,prev_app_features,credit_balance_features,payment_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_list = []\n",
    "for fea in df_features:\n",
    "    if df_features[fea].nunique()==1:\n",
    "        drop_list.append(fea)\n",
    "df_features.drop(labels=drop_list,axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_mem_usage(data, verbose = True):\n",
    "    start_mem = data.memory_usage().sum() / 1024**2\n",
    "    if verbose:\n",
    "        print('Memory usage of dataframe: {:.2f} MB'.format(start_mem))\n",
    "    \n",
    "    for col in data.columns:\n",
    "        col_type = data[col].dtype\n",
    "        \n",
    "        if col_type != object:\n",
    "            c_min = data[col].min()\n",
    "            c_max = data[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    data[col] = data[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    data[col] = data[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    data[col] = data[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    data[col] = data[col].astype(np.int64)  \n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    data[col] = data[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    data[col] = data[col].astype(np.float32)\n",
    "                else:\n",
    "                    data[col] = data[col].astype(np.float64)\n",
    "\n",
    "    end_mem = data.memory_usage().sum() / 1024**2\n",
    "    if verbose:\n",
    "        print('Memory usage after optimization: {:.2f} MB'.format(end_mem))\n",
    "        print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage of dataframe: 2152.65 MB\n",
      "Memory usage after optimization: 756.28 MB\n",
      "Decreased by 64.9%\n"
     ]
    }
   ],
   "source": [
    "df_features = reduce_mem_usage(df_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold = 1 oof score = 0.7853181100678557, train score = 0.8247367398778617 , validate_score = 0.7763126177270324 \n",
      "fold = 2 oof score = 0.789840605183388, train score = 0.8233965558693972 , validate_score = 0.7751277793370306 \n",
      "fold = 3 oof score = 0.7838577366149491, train score = 0.8234448025173577 , validate_score = 0.7761707938602898 \n",
      "fold = 4 oof score = 0.7845047649577565, train score = 0.8246360299841166 , validate_score = 0.7753619392344516 \n",
      "fold = 5 oof score = 0.7761322732588521, train score = 0.8251881630057356 , validate_score = 0.7750261168608406 \n",
      "with seed 42 mean oof = 0.7839306980165602 , std oof = 0.0044272388650565325\n",
      "with validation seed 42 validate score = 0.7771905837055095\n"
     ]
    }
   ],
   "source": [
    "res = validate(data=df_features,nfolds=5,fea_list=[fea for fea in df_features if fea!='TARGET'],\n",
    "               model_type=LGBMClassifier,\n",
    "               param={'n_estimators':300,'max_depth':5,'learning_rate':0.03}) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
